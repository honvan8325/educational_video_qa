{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13755393,"sourceType":"datasetVersion","datasetId":8752645}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e6e91a3b-3488-449b-abe1-683dc802a0a8","cell_type":"code","source":"# !pip install -q transnetv2_pytorch\n# !pip install -Uq transformers==4.46.3 tokenizers==0.20.3\n# !pip install -q open_clip_torch\n# !pip install -q addict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:51:40.336175Z","iopub.execute_input":"2025-11-16T15:51:40.336394Z","iopub.status.idle":"2025-11-16T15:51:40.340139Z","shell.execute_reply.started":"2025-11-16T15:51:40.336375Z","shell.execute_reply":"2025-11-16T15:51:40.339412Z"}},"outputs":[],"execution_count":1},{"id":"ccee3965-44e2-4c87-a048-068a5e8e73d5","cell_type":"code","source":"!rm -rf **","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:51:40.341854Z","iopub.execute_input":"2025-11-16T15:51:40.342048Z","iopub.status.idle":"2025-11-16T15:51:40.507771Z","shell.execute_reply.started":"2025-11-16T15:51:40.342033Z","shell.execute_reply":"2025-11-16T15:51:40.506892Z"}},"outputs":[],"execution_count":2},{"id":"87f8ea0c-60dd-4278-8f2f-dbd4b44a0519","cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nUnified Video Processing Library\nCombines visual (slides) and audio (transcription) processing into context units\n\"\"\"\n\nimport os\nimport gc\nimport json\nimport warnings\nimport subprocess\nimport shutil\nfrom dataclasses import dataclass, asdict\nfrom typing import List, Tuple, Dict, Optional, Any\n\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom pydantic import BaseModel\n\nimport torch\nimport torch.nn.functional as F\n\nfrom scipy.signal import find_peaks\nfrom scipy.ndimage import gaussian_filter1d\nfrom scipy import signal\n\nfrom pydub import AudioSegment\nfrom pydub.utils import make_chunks\nfrom pydub import effects\nfrom pydub.silence import detect_silence\n\nfrom tqdm.auto import tqdm\n\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# ============================================================================\n# Data Structures\n# ============================================================================\n\n\nclass ContextUnitData(BaseModel):\n    \"\"\"Final output data structure for context units\"\"\"\n\n    text: str\n    start_time: float\n    end_time: float\n    visual_text: str = \"\"  # OCR text from slides\n    audio_text: str = \"\"  # Transcribed speech\n\n\n@dataclass\nclass Config:\n    \"\"\"Unified configuration for the pipeline\"\"\"\n\n    # Paths\n    output_dir: str = \"output\"\n    frames_dir: str = \"output/frames\"\n\n    # Phase 1: Frame Extraction\n    target_fps: float = 1.0\n    small_frame_size: Tuple[int, int] = (224, 224)\n    full_frame_size: Tuple[int, int] = (960, 540)\n\n    # Phase 1: Shot Detection (TransNetV2)\n    shot_confidence_threshold: float = 0.5\n\n    # Phase 1: Slide Detection (CLIP ViT-L/14)\n    clip_model: str = \"ViT-L-14\"\n    clip_batch_size: int = 16\n    dynamic_threshold_lambda: float = 0.7\n    smoothing_sigma: float = 2.0\n    min_segment_duration: float = 4.0\n\n    # Phase 2: Keyframe Selection\n    keyframes_per_segment: int = 2\n    quality_weight: float = 0.6\n    diversity_penalty: float = 0.4\n\n    # Phase 3: OCR (DeepSeek-OCR)\n    ocr_model: str = \"deepseek-ai/DeepSeek-OCR\"\n    ocr_base_size: int = 1024\n    ocr_image_size: int = 640\n    ocr_crop_mode: bool = True\n\n    # Audio Processing\n    whisper_model: str = \"vinai/PhoWhisper-medium\"\n    audio_chunk_length_ms: int = 30000\n    noise_reduction: bool = False\n    reduction_strength: float = 0.5\n\n    # Memory Management\n    save_intermediate: bool = True\n\n\n@dataclass\nclass FrameInfo:\n    frame_index: int\n    timestamp: float\n    small_path: str\n    full_path: str\n\n\n@dataclass\nclass ShotInfo:\n    shot_id: int\n    start_frame_idx: int\n    end_frame_idx: int\n    confidence: float\n\n\n@dataclass\nclass SegmentInfo:\n    segment_id: int\n    shot_id: int\n    start_frame_idx: int\n    end_frame_idx: int\n    keyframe_indices: List[int]\n    time_range: Tuple[float, float]\n\n\n@dataclass\nclass TranscriptSegment:\n    start_time: float\n    end_time: float\n    text: str\n\n\n# ============================================================================\n# Utilities\n# ============================================================================\n\n\ndef clear_gpu_memory():\n    \"\"\"Clear GPU memory\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n\n\ndef compute_sharpness(image: np.ndarray) -> float:\n    \"\"\"Compute image sharpness using Laplacian variance\"\"\"\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    else:\n        gray = image\n    return cv2.Laplacian(gray, cv2.CV_64F).var()\n\n\ndef compute_entropy(image: np.ndarray) -> float:\n    \"\"\"Compute Shannon entropy of image\"\"\"\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    else:\n        gray = image\n\n    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n    hist = hist.flatten() / hist.sum()\n    hist = hist[hist > 0]\n    entropy = -np.sum(hist * np.log2(hist))\n    return entropy\n\n\ndef convert_to_serializable(obj):\n    \"\"\"Convert numpy/torch types to JSON-serializable Python types\"\"\"\n    if isinstance(obj, (np.integer, np.int64)):\n        return int(obj)\n    elif isinstance(obj, (np.floating, np.float64)):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    elif isinstance(obj, dict):\n        return {k: convert_to_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, (list, tuple)):\n        return [convert_to_serializable(item) for item in obj]\n    return obj\n\n\ndef save_checkpoint(data: Any, filepath: str):\n    \"\"\"Save intermediate checkpoint\"\"\"\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\n    if isinstance(data, (list, dict)):\n        data = convert_to_serializable(data)\n        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=2, ensure_ascii=False)\n    else:\n        torch.save(data, filepath)\n\n\ndef load_checkpoint(filepath: str) -> Any:\n    \"\"\"Load checkpoint if exists\"\"\"\n    if not os.path.exists(filepath):\n        return None\n\n    if filepath.endswith(\".json\"):\n        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    else:\n        return torch.load(filepath, map_location=\"cpu\")\n\n\n# ============================================================================\n# Audio Extraction & Preprocessing\n# ============================================================================\n\n\ndef extract_audio_from_video(video_path: str, output_folder: str = \"audio\") -> str:\n    \"\"\"Extract audio from video file using ffmpeg\"\"\"\n    try:\n        os.makedirs(output_folder, exist_ok=True)\n\n        if not os.path.exists(video_path):\n            raise FileNotFoundError(f\"Video file not found: {video_path}\")\n\n        base_name = os.path.splitext(os.path.basename(video_path))[0]\n        audio_path = os.path.join(output_folder, f\"{base_name}.wav\")\n\n        if os.path.exists(audio_path):\n            return audio_path\n\n        ffmpeg_cmd = [\n            \"ffmpeg\",\n            \"-i\",\n            video_path,\n            \"-vn\",\n            \"-acodec\",\n            \"pcm_s16le\",\n            \"-ar\",\n            \"16000\",\n            \"-ac\",\n            \"1\",\n            \"-y\",\n            audio_path,\n        ]\n\n        subprocess.run(\n            ffmpeg_cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True\n        )\n\n        return audio_path\n\n    except Exception as e:\n        raise RuntimeError(f\"Audio extraction failed: {str(e)}\")\n\n\ndef reduce_noise_spectral_subtraction(audio_segment, noise_factor=2.0):\n    \"\"\"Safe spectral subtraction that prevents removing entire speech\"\"\"\n    try:\n        samples = np.array(audio_segment.get_array_of_samples())\n        sample_rate = audio_segment.frame_rate\n\n        if audio_segment.channels == 2:\n            samples = samples.reshape((-1, 2))\n            left = _spectral_subtract_channel_safe(\n                samples[:, 0], sample_rate, noise_factor\n            )\n            right = _spectral_subtract_channel_safe(\n                samples[:, 1], sample_rate, noise_factor\n            )\n            cleaned = np.column_stack((left, right)).flatten()\n        else:\n            cleaned = _spectral_subtract_channel_safe(\n                samples, sample_rate, noise_factor\n            )\n\n        cleaned = np.clip(cleaned, -32768, 32767).astype(np.int16)\n\n        cleaned_audio = audio_segment._spawn(cleaned.tobytes())\n        return cleaned_audio\n\n    except Exception as e:\n        return audio_segment\n\n\ndef _spectral_subtract_channel_safe(samples, sr, noise_factor):\n    \"\"\"Safe spectral subtraction: never zeroes out full speech\"\"\"\n    fft = np.fft.rfft(samples)\n    mag = np.abs(fft)\n    phase = np.angle(fft)\n\n    noise_est = np.percentile(mag, 10)\n    clean_mag = mag - noise_factor * noise_est\n    clean_mag = np.maximum(clean_mag, mag * 0.20)\n\n    clean_fft = clean_mag * np.exp(1j * phase)\n    cleaned = np.fft.irfft(clean_fft)\n\n    return cleaned\n\n\ndef apply_high_pass_filter(audio_segment, cutoff_freq=80):\n    \"\"\"Apply high-pass filter to remove low-frequency noise\"\"\"\n    try:\n        samples = np.array(audio_segment.get_array_of_samples())\n        sample_rate = audio_segment.frame_rate\n\n        if audio_segment.channels == 2:\n            samples = samples.reshape((-1, 2))\n            left = _apply_highpass_channel(samples[:, 0], sample_rate, cutoff_freq)\n            right = _apply_highpass_channel(samples[:, 1], sample_rate, cutoff_freq)\n            filtered = np.column_stack((left, right)).flatten()\n        else:\n            filtered = _apply_highpass_channel(samples, sample_rate, cutoff_freq)\n\n        filtered = np.clip(filtered, -32768, 32767).astype(np.int16)\n\n        filtered_audio = audio_segment._spawn(filtered.tobytes())\n        return filtered_audio\n\n    except Exception as e:\n        return audio_segment\n\n\ndef _apply_highpass_channel(samples, sample_rate, cutoff_freq):\n    \"\"\"High-pass for 1 channel\"\"\"\n    try:\n        samples_float = samples.astype(np.float32)\n        nyquist = sample_rate / 2.0\n        normal_cutoff = cutoff_freq / nyquist\n        b, a = signal.butter(5, normal_cutoff, btype=\"high\", analog=False)\n        filtered = signal.filtfilt(b, a, samples_float)\n        return filtered\n    except Exception as e:\n        return samples\n\n\ndef preprocess_audio_with_noise_reduction(\n    audio_path, noise_reduction=True, reduction_strength=0.5\n):\n    \"\"\"Preprocess audio with noise reduction\"\"\"\n    try:\n        audio = AudioSegment.from_file(audio_path)\n\n        if noise_reduction:\n            audio = apply_high_pass_filter(audio, cutoff_freq=80)\n            audio = reduce_noise_spectral_subtraction(\n                audio, noise_factor=reduction_strength * 3.0\n            )\n\n        if audio.dBFS < -30.0:\n            boost_amount = min(-20.0 - audio.dBFS, 15.0)\n            audio = audio.apply_gain(boost_amount)\n\n        audio = audio.set_frame_rate(16000).set_channels(1)\n\n        return audio\n\n    except Exception as e:\n        return AudioSegment.from_file(audio_path)\n\n\ndef split_audio(audio_path: str, chunk_length_ms: int = 30000) -> List[str]:\n    \"\"\"Split audio into chunks\"\"\"\n    try:\n        audio = AudioSegment.from_file(audio_path)\n        chunks = make_chunks(audio, chunk_length_ms)\n\n        chunk_paths = []\n        for i, chunk in enumerate(chunks):\n            chunk_path = f\"temp_chunk_{i}.wav\"\n            chunk.export(\n                chunk_path, format=\"wav\", parameters=[\"-ac\", \"1\", \"-ar\", \"16000\"]\n            )\n            chunk_paths.append(chunk_path)\n\n        return chunk_paths\n\n    except Exception as e:\n        return [audio_path]\n\n\n# ============================================================================\n# Phase 1: Frame Extraction\n# ============================================================================\n\n\nclass VideoFrameExtractor:\n    \"\"\"Extract and preprocess frames from video\"\"\"\n\n    def __init__(self, config: Config):\n        self.config = config\n        os.makedirs(config.frames_dir, exist_ok=True)\n\n    def extract_frames(self, video_path: str) -> List[FrameInfo]:\n        \"\"\"Extract frames at target FPS\"\"\"\n        checkpoint_path = os.path.join(self.config.output_dir, \"frames_info.json\")\n        if self.config.save_intermediate and os.path.exists(checkpoint_path):\n            frames_data = load_checkpoint(checkpoint_path)\n            frames_info = [FrameInfo(**f) for f in frames_data]\n            return frames_info\n\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            raise ValueError(f\"Cannot open video: {video_path}\")\n\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        frame_interval = int(fps / self.config.target_fps)\n\n        frames_info = []\n        frame_count = 0\n\n        pbar = tqdm(total=total_frames, desc=\"Extracting frames\")\n\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            if frame_count % frame_interval == 0:\n                frame_index = len(frames_info)\n                timestamp = frame_count / fps\n\n                small_frame = cv2.resize(frame, self.config.small_frame_size)\n                small_path = os.path.join(\n                    self.config.frames_dir, f\"frame_{frame_index:05d}_small.jpg\"\n                )\n                cv2.imwrite(small_path, small_frame)\n\n                full_frame = cv2.resize(frame, self.config.full_frame_size)\n                full_path = os.path.join(\n                    self.config.frames_dir, f\"frame_{frame_index:05d}_full.jpg\"\n                )\n                cv2.imwrite(full_path, full_frame)\n\n                frames_info.append(\n                    FrameInfo(\n                        frame_index=frame_index,\n                        timestamp=timestamp,\n                        small_path=small_path,\n                        full_path=full_path,\n                    )\n                )\n\n            frame_count += 1\n            pbar.update(1)\n\n        pbar.close()\n        cap.release()\n\n        if self.config.save_intermediate:\n            frames_data = [asdict(f) for f in frames_info]\n            save_checkpoint(frames_data, checkpoint_path)\n\n        return frames_info\n\n\n# ============================================================================\n# Phase 1b: Shot Boundary Detection\n# ============================================================================\n\n\nclass ShotDetector:\n    \"\"\"Detect shot boundaries using TransNetV2\"\"\"\n\n    def __init__(self, config: Config):\n        self.config = config\n        self.model = None\n\n    def load_model(self):\n        \"\"\"Load TransNetV2 model\"\"\"\n        from transnetv2_pytorch import TransNetV2\n\n        self.model = TransNetV2()\n        if torch.cuda.is_available():\n            self.model = self.model.to(device)\n        self.model.eval()\n\n    def detect_shots(\n        self, video_path: str, frames_info: List[FrameInfo]\n    ) -> List[ShotInfo]:\n        \"\"\"Detect shot boundaries from frames\"\"\"\n        checkpoint_path = os.path.join(self.config.output_dir, \"shots_info.json\")\n        if self.config.save_intermediate and os.path.exists(checkpoint_path):\n            shots_data = load_checkpoint(checkpoint_path)\n            shots = [ShotInfo(**s) for s in shots_data]\n            return shots\n\n        if self.model is None:\n            self.load_model()\n\n        _, single_predictions, _ = self.model.predict_video(video_path)\n        predictions = single_predictions.cpu().numpy()\n\n        cap = cv2.VideoCapture(video_path)\n        original_fps = cap.get(cv2.CAP_PROP_FPS)\n        cap.release()\n\n        frame_interval = int(original_fps / self.config.target_fps)\n\n        shot_boundaries = []\n        for i in range(len(predictions)):\n            if predictions[i] > self.config.shot_confidence_threshold:\n                sampled_idx = i // frame_interval\n                if (\n                    sampled_idx < len(frames_info)\n                    and sampled_idx not in shot_boundaries\n                ):\n                    shot_boundaries.append(sampled_idx)\n\n        boundaries = [0] + shot_boundaries + [len(frames_info) - 1]\n        shots = []\n\n        for i in range(len(boundaries) - 1):\n            start_idx = boundaries[i]\n            end_idx = boundaries[i + 1]\n\n            if i < len(shot_boundaries):\n                original_idx = shot_boundaries[i] * frame_interval\n                if original_idx < len(predictions):\n                    confidence = float(predictions[original_idx])\n                else:\n                    confidence = 0.0\n            else:\n                confidence = 0.0\n\n            shots.append(\n                ShotInfo(\n                    shot_id=i,\n                    start_frame_idx=start_idx,\n                    end_frame_idx=end_idx,\n                    confidence=confidence,\n                )\n            )\n\n        if self.config.save_intermediate:\n            shots_data = [asdict(s) for s in shots]\n            save_checkpoint(shots_data, checkpoint_path)\n\n        return shots\n\n    def free_model(self):\n        \"\"\"Free TransNetV2 model from memory\"\"\"\n        if self.model is not None:\n            del self.model\n            self.model = None\n            clear_gpu_memory()\n\n\n# ============================================================================\n# Phase 1c: CLIP Slide Detection\n# ============================================================================\n\n\nclass CLIPSlideDetector:\n    \"\"\"Detect slide boundaries within shots using CLIP ViT-L/14\"\"\"\n\n    def __init__(self, config: Config):\n        self.config = config\n        self.model = None\n        self.preprocess = None\n\n    def load_model(self):\n        \"\"\"Load CLIP ViT-L/14 model\"\"\"\n        import open_clip\n\n        self.model, _, self.preprocess = open_clip.create_model_and_transforms(\n            \"ViT-L-14\", pretrained=\"openai\"\n        )\n        self.model = self.model.to(device)\n        self.model.eval()\n\n    def extract_clip_features(self, frames_info: List[FrameInfo]) -> torch.Tensor:\n        \"\"\"Extract CLIP features for all frames\"\"\"\n        checkpoint_path = os.path.join(self.config.output_dir, \"clip_features.pt\")\n        if self.config.save_intermediate and os.path.exists(checkpoint_path):\n            features = load_checkpoint(checkpoint_path)\n            return features\n\n        if self.model is None:\n            self.load_model()\n\n        features_list = []\n\n        for i in tqdm(\n            range(0, len(frames_info), self.config.clip_batch_size),\n            desc=\"CLIP encoding\",\n        ):\n            batch_frames = frames_info[i : i + self.config.clip_batch_size]\n\n            images = []\n            for frame_info in batch_frames:\n                img = Image.open(frame_info.small_path).convert(\"RGB\")\n                images.append(self.preprocess(img))\n\n            images_tensor = torch.stack(images).to(device)\n\n            with torch.no_grad():\n                features = self.model.encode_image(images_tensor)\n                features = F.normalize(features, dim=-1)\n\n            features_list.append(features.cpu())\n\n        all_features = torch.cat(features_list, dim=0)\n\n        if self.config.save_intermediate:\n            save_checkpoint(all_features, checkpoint_path)\n\n        return all_features\n\n    def detect_slide_boundaries_in_shot(\n        self, features: torch.Tensor, shot_start: int, shot_end: int\n    ) -> List[int]:\n        \"\"\"Detect slide boundaries within a shot using dynamic thresholding\"\"\"\n        shot_features = features[shot_start : shot_end + 1]\n\n        if len(shot_features) < 2:\n            return []\n\n        similarities = []\n        for i in range(len(shot_features) - 1):\n            sim = F.cosine_similarity(\n                shot_features[i : i + 1], shot_features[i + 1 : i + 2], dim=1\n            )\n            similarities.append(sim.item())\n\n        similarities = np.array(similarities)\n        dissimilarity = 1 - similarities\n\n        dissimilarity_smooth = gaussian_filter1d(\n            dissimilarity, sigma=self.config.smoothing_sigma\n        )\n\n        mean_dissim = dissimilarity_smooth.mean()\n        std_dissim = dissimilarity_smooth.std()\n        threshold = mean_dissim + self.config.dynamic_threshold_lambda * std_dissim\n\n        peaks, _ = find_peaks(dissimilarity_smooth, height=threshold)\n        boundaries = [shot_start + p for p in peaks]\n\n        return boundaries\n\n    def merge_short_segments(\n        self, segments: List[SegmentInfo], frames_info: List[FrameInfo]\n    ) -> List[SegmentInfo]:\n        \"\"\"Merge segments shorter than min_duration\"\"\"\n        if len(segments) <= 1:\n            return segments\n\n        merged = []\n        i = 0\n\n        while i < len(segments):\n            current = segments[i]\n            duration = current.time_range[1] - current.time_range[0]\n\n            if duration >= self.config.min_segment_duration:\n                merged.append(current)\n                i += 1\n            else:\n                if i + 1 < len(segments):\n                    next_seg = segments[i + 1]\n                    merged_seg = SegmentInfo(\n                        segment_id=current.segment_id,\n                        shot_id=current.shot_id,\n                        start_frame_idx=current.start_frame_idx,\n                        end_frame_idx=next_seg.end_frame_idx,\n                        keyframe_indices=[],\n                        time_range=(current.time_range[0], next_seg.time_range[1]),\n                    )\n                    merged.append(merged_seg)\n                    i += 2\n                elif len(merged) > 0:\n                    prev_seg = merged.pop()\n                    merged_seg = SegmentInfo(\n                        segment_id=prev_seg.segment_id,\n                        shot_id=prev_seg.shot_id,\n                        start_frame_idx=prev_seg.start_frame_idx,\n                        end_frame_idx=current.end_frame_idx,\n                        keyframe_indices=[],\n                        time_range=(prev_seg.time_range[0], current.time_range[1]),\n                    )\n                    merged.append(merged_seg)\n                    i += 1\n                else:\n                    merged.append(current)\n                    i += 1\n\n        for idx, seg in enumerate(merged):\n            seg.segment_id = idx\n\n        return merged\n\n    def select_keyframes_greedy(\n        self,\n        frames_info: List[FrameInfo],\n        features: torch.Tensor,\n        start_idx: int,\n        end_idx: int,\n    ) -> List[int]:\n        \"\"\"Select keyframes using greedy quality + diversity algorithm\"\"\"\n        segment_frames = frames_info[start_idx : end_idx + 1]\n        segment_features = features[start_idx : end_idx + 1]\n\n        if len(segment_frames) == 0:\n            return []\n        if len(segment_frames) == 1:\n            return [start_idx]\n\n        quality_scores = []\n        for frame_info in segment_frames:\n            img = cv2.imread(frame_info.full_path)\n            sharpness = compute_sharpness(img)\n            entropy = compute_entropy(img)\n            quality = 0.6 * sharpness + 0.4 * entropy\n            quality_scores.append(quality)\n\n        quality_scores = np.array(quality_scores)\n\n        if quality_scores.std() > 0:\n            quality_scores = (\n                quality_scores - quality_scores.mean()\n            ) / quality_scores.std()\n            quality_scores = (quality_scores - quality_scores.min()) / (\n                quality_scores.max() - quality_scores.min()\n            )\n        else:\n            quality_scores = np.ones_like(quality_scores)\n\n        selected_indices = []\n        k = min(self.config.keyframes_per_segment, len(segment_frames))\n\n        first_idx = int(np.argmax(quality_scores))\n        selected_indices.append(first_idx)\n\n        for _ in range(k - 1):\n            best_score = -float(\"inf\")\n            best_idx = -1\n\n            for idx in range(len(segment_frames)):\n                if idx in selected_indices:\n                    continue\n\n                max_similarity = 0.0\n                for sel_idx in selected_indices:\n                    sim = F.cosine_similarity(\n                        segment_features[idx : idx + 1],\n                        segment_features[sel_idx : sel_idx + 1],\n                        dim=1,\n                    ).item()\n                    max_similarity = max(max_similarity, sim)\n\n                score = (\n                    self.config.quality_weight * quality_scores[idx]\n                    - self.config.diversity_penalty * max_similarity\n                )\n\n                if score > best_score:\n                    best_score = score\n                    best_idx = idx\n\n            if best_idx >= 0:\n                selected_indices.append(best_idx)\n\n        keyframe_indices = sorted([start_idx + idx for idx in selected_indices])\n\n        return keyframe_indices\n\n    def detect_segments(\n        self, frames_info: List[FrameInfo], shots: List[ShotInfo]\n    ) -> List[SegmentInfo]:\n        \"\"\"Main pipeline for slide segment detection within shots\"\"\"\n        features = self.extract_clip_features(frames_info)\n\n        segments = []\n\n        for shot in tqdm(shots, desc=\"Processing shots\"):\n            boundaries = self.detect_slide_boundaries_in_shot(\n                features, shot.start_frame_idx, shot.end_frame_idx\n            )\n\n            shot_boundaries = [shot.start_frame_idx] + boundaries + [shot.end_frame_idx]\n\n            for i in range(len(shot_boundaries) - 1):\n                start_idx = shot_boundaries[i]\n                end_idx = shot_boundaries[i + 1]\n\n                segments.append(\n                    SegmentInfo(\n                        segment_id=len(segments),\n                        shot_id=shot.shot_id,\n                        start_frame_idx=start_idx,\n                        end_frame_idx=end_idx,\n                        keyframe_indices=[],\n                        time_range=(\n                            frames_info[start_idx].timestamp,\n                            frames_info[end_idx].timestamp,\n                        ),\n                    )\n                )\n\n        segments = self.merge_short_segments(segments, frames_info)\n\n        for segment in tqdm(segments, desc=\"Keyframe selection\"):\n            keyframes = self.select_keyframes_greedy(\n                frames_info, features, segment.start_frame_idx, segment.end_frame_idx\n            )\n            segment.keyframe_indices = keyframes\n\n        if self.config.save_intermediate:\n            segments_data = [asdict(s) for s in segments]\n            save_checkpoint(\n                segments_data,\n                os.path.join(self.config.output_dir, \"segments_info.json\"),\n            )\n\n        return segments\n\n    def free_model(self):\n        \"\"\"Free CLIP model from memory\"\"\"\n        if self.model is not None:\n            del self.model\n            del self.preprocess\n            self.model = None\n            self.preprocess = None\n            clear_gpu_memory()\n\n\n# ============================================================================\n# Phase 3: OCR (DeepSeek-OCR)\n# ============================================================================\n\n\nclass DeepSeekProcessor:\n    \"\"\"Extract text using DeepSeek-OCR\"\"\"\n\n    def __init__(self, config: Config):\n        self.config = config\n        self.model = None\n        self.tokenizer = None\n\n    def load_model(self):\n        \"\"\"Load DeepSeek-OCR model\"\"\"\n        from transformers import AutoModel, AutoTokenizer\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.config.ocr_model, trust_remote_code=True\n        )\n        self.model = AutoModel.from_pretrained(\n            self.config.ocr_model, trust_remote_code=True, use_safetensors=True\n        )\n        self.model = self.model.eval().cuda().to(torch.bfloat16)\n\n    def process_image_unified(self, image_path: str) -> str:\n        \"\"\"Extract OCR text from image\"\"\"\n        if self.model is None:\n            self.load_model()\n\n        try:\n            prompt = \"<image>\\nDescribe this image in detail.\"\n\n            self.model.infer(\n                self.tokenizer,\n                prompt=prompt,\n                image_file=image_path,\n                output_path=self.config.output_dir,\n                base_size=self.config.ocr_base_size,\n                image_size=self.config.ocr_image_size,\n                crop_mode=self.config.ocr_crop_mode,\n                save_results=True,\n                test_compress=True,\n            )\n\n            result_file = os.path.join(self.config.output_dir, \"result.mmd\")\n            if os.path.exists(result_file):\n                with open(result_file, \"r\", encoding=\"utf-8\") as f:\n                    result_text = f.read().strip()\n            else:\n                result_text = \"\"\n\n            return result_text\n\n        except Exception as e:\n            return \"\"\n\n    def process_segments_unified(\n        self, frames_info: List[FrameInfo], segments: List[SegmentInfo]\n    ) -> Dict[int, str]:\n        \"\"\"Process OCR - extract text from first keyframe of each segment\"\"\"\n        checkpoint = os.path.join(self.config.output_dir, \"segment_results.json\")\n\n        if self.config.save_intermediate and os.path.exists(checkpoint):\n            segment_results = load_checkpoint(checkpoint)\n            segment_results = {int(k): v for k, v in segment_results.items()}\n            return segment_results\n\n        if self.model is None:\n            self.load_model()\n\n        segment_results = {}\n\n        for segment in tqdm(segments, desc=\"OCR processing\"):\n            if segment.keyframe_indices:\n                frame_info = frames_info[segment.keyframe_indices[0]]\n                text = self.process_image_unified(frame_info.full_path)\n                segment_results[segment.segment_id] = text\n            else:\n                segment_results[segment.segment_id] = \"\"\n\n        if self.config.save_intermediate:\n            save_checkpoint(segment_results, checkpoint)\n\n        return segment_results\n\n    def free_model(self):\n        \"\"\"Free DeepSeek-OCR model from memory\"\"\"\n        if self.model is not None:\n            del self.model\n            self.model = None\n        if self.tokenizer is not None:\n            del self.tokenizer\n            self.tokenizer = None\n        clear_gpu_memory()\n\n\n# ============================================================================\n# Audio Transcription\n# ============================================================================\n\n\nclass AudioTranscriber:\n    \"\"\"Transcribe audio using Whisper\"\"\"\n\n    def __init__(self, config: Config):\n        self.config = config\n        self.transcriber = None\n\n    def load_model(self):\n        \"\"\"Load Whisper model\"\"\"\n        from transformers import pipeline\n\n        self.transcriber = pipeline(\n            \"automatic-speech-recognition\",\n            model=self.config.whisper_model,\n            device=device.type,\n            return_timestamps=True,\n            framework=\"pt\",\n            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n            model_kwargs={\"use_cache\": True},\n        )\n\n    def transcribe_audio(self, audio_path: str) -> List[TranscriptSegment]:\n        \"\"\"Transcribe audio with timestamps\"\"\"\n        if self.transcriber is None:\n            self.load_model()\n\n        print(f\"[DEBUG] Starting audio transcription from: {audio_path}\")\n\n        try:\n            if self.config.noise_reduction:\n                print(\"[DEBUG] Applying noise reduction...\")\n                audio = preprocess_audio_with_noise_reduction(\n                    audio_path,\n                    noise_reduction=True,\n                    reduction_strength=self.config.reduction_strength,\n                )\n                processed_audio_path = \"temp_processed_audio.wav\"\n                audio.export(processed_audio_path, format=\"wav\")\n                audio_path = processed_audio_path\n\n            chunk_paths = split_audio(audio_path, self.config.audio_chunk_length_ms)\n            print(f\"[DEBUG] Split audio into {len(chunk_paths)} chunks\")\n\n            all_segments = []\n            current_offset = 0.0\n\n            for idx, chunk_path in enumerate(\n                tqdm(chunk_paths, desc=\"Transcribing audio\")\n            ):\n                try:\n                    print(f\"[DEBUG] Processing chunk {idx+1}/{len(chunk_paths)}...\")\n                    result = self.transcriber(\n                        chunk_path,\n                        return_timestamps=True,\n                        generate_kwargs={\n                            \"language\": \"vietnamese\",\n                            \"task\": \"transcribe\",\n                        },\n                    )\n\n                    print(f\"[DEBUG] Chunk {idx+1} result keys: {result.keys()}\")\n\n                    if \"chunks\" in result:\n                        print(\n                            f\"[DEBUG] Found {len(result['chunks'])} text chunks in audio chunk {idx+1}\"\n                        )\n                        for chunk in result[\"chunks\"]:\n                            timestamp = chunk.get(\"timestamp\", (0, 0))\n                            start_time = current_offset + (\n                                timestamp[0] if timestamp[0] is not None else 0\n                            )\n                            end_time = current_offset + (\n                                timestamp[1] if timestamp[1] is not None else 0\n                            )\n                            text = chunk.get(\"text\", \"\").strip()\n\n                            if text:\n                                print(\n                                    f\"[DEBUG] Audio segment: [{start_time:.1f}s - {end_time:.1f}s] {text[:50]}...\"\n                                )\n                                all_segments.append(\n                                    TranscriptSegment(\n                                        start_time=start_time,\n                                        end_time=end_time,\n                                        text=text,\n                                    )\n                                )\n                    else:\n                        print(f\"[DEBUG] No 'chunks' key in result for chunk {idx+1}\")\n\n                    current_offset += self.config.audio_chunk_length_ms / 1000.0\n\n                except Exception as e:\n                    print(f\"[DEBUG] Error processing chunk {idx+1}: {str(e)}\")\n                    current_offset += self.config.audio_chunk_length_ms / 1000.0\n                    continue\n\n            # Cleanup temp files\n            for chunk_path in chunk_paths:\n                if os.path.exists(chunk_path) and chunk_path.startswith(\"temp_\"):\n                    os.remove(chunk_path)\n\n            if self.config.noise_reduction and os.path.exists(\n                \"temp_processed_audio.wav\"\n            ):\n                os.remove(\"temp_processed_audio.wav\")\n\n            print(f\"[DEBUG] Total audio segments extracted: {len(all_segments)}\")\n            return all_segments\n\n        except Exception as e:\n            print(f\"[DEBUG] Audio transcription failed: {str(e)}\")\n            return []\n\n    def free_model(self):\n        \"\"\"Free Whisper model from memory\"\"\"\n        if self.transcriber is not None:\n            del self.transcriber\n            self.transcriber = None\n            clear_gpu_memory()\n\n\n# ============================================================================\n# Merging Logic\n# ============================================================================\n\n\ndef merge_visual_audio(\n    segments: List[SegmentInfo],\n    visual_texts: Dict[int, str],\n    audio_segments: List[TranscriptSegment],\n) -> List[ContextUnitData]:\n    \"\"\"\n    Merge visual and audio data into context units.\n    Visual segments define the structure, audio is merged based on time overlap.\n    \"\"\"\n    print(\n        f\"\\n[DEBUG] Merging {len(segments)} visual segments with {len(audio_segments)} audio segments\"\n    )\n\n    context_units = []\n\n    for segment in segments:\n        v_start, v_end = segment.time_range\n        visual_text = visual_texts.get(segment.segment_id, \"\")\n\n        # Find overlapping audio segments\n        overlapping_audio = []\n        for audio_seg in audio_segments:\n            # Check if audio segment overlaps with visual segment\n            if audio_seg.start_time < v_end and audio_seg.end_time > v_start:\n                overlapping_audio.append(audio_seg.text)\n\n        # Concatenate texts\n        audio_text = \" \".join(overlapping_audio)\n\n        print(f\"[DEBUG] Segment {segment.segment_id} [{v_start:.1f}s - {v_end:.1f}s]:\")\n        print(f\"  - Visual text length: {len(visual_text)}\")\n        print(f\"  - Audio segments matched: {len(overlapping_audio)}\")\n        print(f\"  - Audio text length: {len(audio_text)}\")\n\n        # Build combined text with clear separators\n        combined_parts = []\n        \n        if visual_text:\n            combined_parts.append(f\"[VISUAL]\\n{visual_text}\")\n        \n        if audio_text:\n            combined_parts.append(f\"[AUDIO]\\n{audio_text}\")\n        \n        combined_text = \"\\n\\n\".join(combined_parts) if combined_parts else \"\"\n\n        context_units.append(\n            ContextUnitData(\n                text=combined_text,\n                start_time=v_start,\n                end_time=v_end,\n                visual_text=visual_text,\n                audio_text=audio_text,\n            )\n        )\n\n    print(f\"[DEBUG] Created {len(context_units)} context units\")\n    return context_units\n\n\n# ============================================================================\n# Main Processing Function\n# ============================================================================\n\n\ndef process_video(\n    video_path: str, config: Optional[Config] = None\n) -> List[ContextUnitData]:\n    \"\"\"\n    Main function to process video and return context units.\n\n    Args:\n        video_path: Path to the video file\n        config: Optional configuration (uses defaults if not provided)\n\n    Returns:\n        List of ContextUnitData with merged visual and audio information\n    \"\"\"\n    if config is None:\n        config = Config()\n\n    os.makedirs(config.output_dir, exist_ok=True)\n    os.makedirs(config.frames_dir, exist_ok=True)\n\n    # Extract audio from video\n    audio_path = extract_audio_from_video(\n        video_path, os.path.join(config.output_dir, \"audio\")\n    )\n\n    # Phase 1: Frame Extraction\n    extractor = VideoFrameExtractor(config)\n    frames_info = extractor.extract_frames(video_path)\n\n    # Phase 1b: Shot Detection\n    shot_detector = ShotDetector(config)\n    shots = shot_detector.detect_shots(video_path, frames_info)\n    shot_detector.free_model()\n\n    # Phase 1c: Slide Detection & Keyframe Selection\n    slide_detector = CLIPSlideDetector(config)\n    segments = slide_detector.detect_segments(frames_info, shots)\n    slide_detector.free_model()\n\n    # Phase 3: OCR Processing\n    deepseek_processor = DeepSeekProcessor(config)\n    visual_texts = deepseek_processor.process_segments_unified(frames_info, segments)\n    deepseek_processor.free_model()\n\n    # Phase 4: Audio Transcription\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Phase 4: Audio Transcription\")\n    print(\"=\" * 60)\n    audio_transcriber = AudioTranscriber(config)\n    audio_segments = audio_transcriber.transcribe_audio(audio_path)\n    print(f\"[INFO] Extracted {len(audio_segments)} audio segments\")\n    audio_transcriber.free_model()\n\n    # Phase 5: Merge Visual and Audio\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Phase 5: Merging Visual and Audio\")\n    print(\"=\" * 60)\n    context_units = merge_visual_audio(segments, visual_texts, audio_segments)\n\n    return context_units\n\n\n# ============================================================================\n# Export Functions\n# ============================================================================\n\n\ndef save_context_units(context_units: List[ContextUnitData], output_path: str):\n    \"\"\"Save context units to JSON file\"\"\"\n    data = [unit.dict() for unit in context_units]\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n\ndef load_context_units(input_path: str) -> List[ContextUnitData]:\n    \"\"\"Load context units from JSON file\"\"\"\n    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    return [ContextUnitData(**unit) for unit in data]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:51:40.509009Z","iopub.execute_input":"2025-11-16T15:51:40.509320Z","iopub.status.idle":"2025-11-16T15:51:44.404543Z","shell.execute_reply.started":"2025-11-16T15:51:40.509293Z","shell.execute_reply":"2025-11-16T15:51:44.403662Z"}},"outputs":[],"execution_count":3},{"id":"f98ef6e2-0467-46e4-9ce7-f0c247ce5e07","cell_type":"code","source":"%%time\n# ============================================================================\n# Constants\n# ============================================================================\n\n# Configure these constants before running\nVIDEO_PATH = \"/kaggle/input/cs431videos/6.3. CS431 - Chuong 6 Part 3 Bieu dien tu bang Vector O57P9YHZOE0.mp4\"\nOUTPUT_DIR = \"output\"\n\n\n# ============================================================================\n# Main Execution\n# ============================================================================\n\n\nif __name__ == \"__main__\":\n    # Create config\n    config = Config(\n        output_dir=OUTPUT_DIR,\n        frames_dir=f\"{OUTPUT_DIR}/frames\",\n        noise_reduction=True,\n        reduction_strength=0.5,\n        save_intermediate=True\n    )\n    \n    print(f\"Processing video: {VIDEO_PATH}\")\n    print(f\"Output directory: {OUTPUT_DIR}\")\n    print(\"=\"*60)\n    \n    # Process video\n    context_units = process_video(VIDEO_PATH, config)\n    \n    # Save results\n    output_path = os.path.join(OUTPUT_DIR, \"context_units.json\")\n    save_context_units(context_units, output_path)\n    \n    print(\"=\"*60)\n    print(f\"Processing complete!\")\n    print(f\"Total context units: {len(context_units)}\")\n    print(f\"Results saved to: {output_path}\")\n    \n    # Statistics\n    units_with_visual = sum(1 for u in context_units if u.visual_text)\n    units_with_audio = sum(1 for u in context_units if u.audio_text)\n    units_with_both = sum(1 for u in context_units if u.visual_text and u.audio_text)\n    \n    print(f\"\\nStatistics:\")\n    print(f\"  - Units with visual text: {units_with_visual}\")\n    print(f\"  - Units with audio text: {units_with_audio}\")\n    print(f\"  - Units with both: {units_with_both}\")\n    \n    print(\"\\nSample units:\")\n    for i, unit in enumerate(context_units[:3]):\n        print(f\"\\nUnit {i+1} [{unit.start_time:.1f}s - {unit.end_time:.1f}s]:\")\n        print(f\"  Visual: {unit.visual_text}\")\n        print(f\"  Audio: {unit.audio_text}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:51:44.405460Z","iopub.execute_input":"2025-11-16T15:51:44.405829Z"}},"outputs":[{"name":"stdout","text":"Processing video: /kaggle/input/cs431videos/6.3. CS431 - Chuong 6 Part 3 Bieu dien tu bang Vector O57P9YHZOE0.mp4\nOutput directory: output\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting frames:   0%|          | 0/34403 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c4d5625bca44c4096f43ad70942ab54"}},"metadata":{}},{"name":"stdout","text":"Extracting frames from /kaggle/input/cs431videos/6.3. CS431 - Chuong 6 Part 3 Bieu dien tu bang Vector O57P9YHZOE0.mp4\n","output_type":"stream"},{"name":"stderr","text":"Processing frames: 100%|██████████| 34403/34403 [00:21<00:00, 1573.06frame/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"CLIP encoding:   0%|          | 0/36 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"793aa01890084d1bb66d943ec1d0165d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing shots:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b08f2f781ef4044ac3e14d83535d61d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Keyframe selection:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffc7dbf20bea4d6cbdf48355461ce0e9"}},"metadata":{}},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763308528.078240    8519 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763308528.084893    8519 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nSome weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"OCR processing:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe6d6583d9fe422a897ddf0bd5dd2bee"}},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nThe `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\nThe attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([2, 100, 1280])\n=====================\nThe image is a digital presentation slide with a white background and a combination of text and graphics. At the top, there is a blue banner with the text \"NỘI DUNG\" in white capital letters. Below the banner, there are three numbered points in black text:\n\n1. \"XỬ LÝ NGÔN NGỮ TỰ NHIÊN\"\n2. \"HỌC SÂU TRONG XỬ LÝ NGÔN NGỮ TỰ NHIÊN\"\n3. \"BIỂU DIỄN TỪ VỚI VECTOR\"\n\nIn the center of the slide, there is a graphic of a blue banner with the text \"TS. Nguyễn Vinh Tiệp\" in white, followed by \"Giảng viên Khoa Khoa học Máy tính\" in smaller font size. The banner also features a small graphic of a globe with a blue and white color scheme.\n\nOn the right side of the slide, there is a photograph of a man wearing a light-colored shirt with a dark tie. He is standing in front of a plain background and appears to be speaking or presenting. The man has short black hair and is looking slightly to his left with a neutral expression.\n\nThe slide also includes a footer with the text \"Thực hiện bởi Trường Đại học Công nghệ Thông tin, ĐHQG-HCM\" in small black font, indicating the source or affiliation of the presentation.\n\nThe overall layout of the slide is clean and professional, with a clear focus on the textual information provided. The use of blue and white colors is consistent throughout the slide, providing a sense of unity and coherence.\n==================================================\nimage size:  (960, 540)\nvalid image tokens:  344\noutput texts tokens (valid):  359\ncompression ratio:  1.04\n==================================================\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([2, 100, 1280])\n=====================\nThe image displays a presentation slide with a blue and white color scheme. At the top, there is a logo consisting of a stylized letter 'B' with a circular element, followed by the text \"Biểu diễn từ bằng vector\" in bold, dark blue font. Below this title, there are two bullet points in a lighter blue font. The first bullet point states \"Biểu diễn từ bằng vector rất quan trọng khi áp dụng vào các mô hình máy học,\" which translates to \"Vector representation is very important when applying it to various machine learning models.\" The second bullet point reads \"Các kỹ thuật thường được sử dụng: One-hot vector, Bag-of-words hay BOW,\" meaning \"Common techniques used include One-hot vector, Bag-of-words, and BOW.\"\n\nOn the right side of the slide, there is a photograph of a man wearing a light blue shirt and a dark blue tie. He has short black hair, is smiling, and appears to be speaking or presenting. The man is standing in front of a blurred background that does not provide any additional context.\n\nThe bottom of the slide features a footer with the text \"Thực hiện bởi Trường Đại học Công nghệ Thông tin, ĐHQG-HCM,\" which translates to \"Implemented by the Faculty of Information Technology, Ho Chi Minh City University of Technology.\" The slide is numbered \"12\" in the bottom right corner, indicating its sequence within the presentation.\n==================================================\nimage size:  (960, 540)\nvalid image tokens:  344\noutput texts tokens (valid):  314\ncompression ratio:  0.91\n==================================================\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([2, 100, 1280])\n=====================\nThe image displays a presentation slide with a blue and white color scheme. At the top, there is a logo consisting of a blue circle with a white swirl design inside it. Below the logo, the slide is titled \"Biểu diễn từ bằng vector\" in bold, black font.\n\nThe slide contains bullet points in Vietnamese, which translate to \"1. Biểu diễn từ bằng vector rất quan trọng khi áp dụng vào các mô hình máy học\" and \"2. Các kỹ thuật thường được sử dụng: One-hot vector, Bag-of-words hay BOW. Biểu diễn bằng ngữ cảnh.\"\n\nOn the right side of the slide, there is a photograph of a man wearing a light blue shirt with a collar. He has short black hair, is looking slightly to his left with a neutral expression, and is wearing a dark tie. The man is standing in front of a blurred background that does not provide any additional context.\n\nAt the bottom of the slide, there is a footer in blue text that reads \"Thực hiện bởi Trường Đại học Công nghệ Thông tin, ĐHQG-HCM,\" which translates to \"Implemented by the Faculty of Information Technology, Hanoi National University of Industry.\"\n\nThe slide appears to be part of an educational or informational presentation, possibly related to machine learning or data science, given the references to vector representations and machine learning techniques.\n==================================================\nimage size:  (960, 540)\nvalid image tokens:  344\noutput texts tokens (valid):  302\ncompression ratio:  0.88\n==================================================\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([2, 100, 1280])\n=====================\nThe image displays a presentation slide with a blue and white color scheme. At the top, there is a logo consisting of a blue circle with a white swirl design inside it. Below the logo, the slide is titled \"Biểu diễn từ bằng vector\" in bold, black font.\n\nThe slide contains bullet points in Vietnamese, which translate to \"1. Biểu diễn từ bằng vector rất quan trọng khi áp dụng vào các mô hình máy học\" and \"2. Các kỹ thuật thường được sử dụng: One-hot vector, Bag-of-words hay BOW. Biểu diễn bằng ngữ cảnh.\"\n\nOn the right side of the slide, there is a photograph of a man wearing a light blue shirt with a collar. He has short black hair, is smiling, and looking directly at the camera. The man appears to be standing in an indoor setting with a blurred background that does not provide any additional context.\n\nThe bottom of the slide includes a footer in blue text that reads \"Thực hiện bởi Trường Đại học Công nghệ Thông tin, ĐHQG-HCM,\" which translates to \"Implemented by the Faculty of Information Technology, Hanoi National University of Industry.\"\n\nThe overall layout of the slide is professional, with a clear focus on the textual information provided. The photograph of the man adds a human element to the presentation.\n==================================================\nimage size:  (960, 540)\nvalid image tokens:  344\noutput texts tokens (valid):  294\ncompression ratio:  0.85\n==================================================\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([2, 100, 1280])\n=====================\nThe image displays a presentation slide with a title \"Cách 1: Biểu diễn với One-hot vector\" which translates to \"Method 1: Representing with One-hot vector\". The slide is divided into two main sections.\n\nOn the left side, there is a bullet-pointed list with the following text: \"Trước đây, từ được xem là một phần tử trong một trường hợp đặc biệt.\" This translates to \"Previously, we have seen that a vector is a part of a vector space. In this special case.\"\n\nBelow this list, there is a mathematical expression: \"Một số 1, bởi 1, bởi 0 → y nghĩa: vị trí của từ trong trường hợp đặc biệt.\" This translates to \"One vector, by 1, by 0 → the meaning of the vector is: the position of the vector in the special case.\"\n\nOn the right side, there is a diagram with a red arrow pointing from a vector labeled \"One-hot vector\" to a mathematical expression \"Một số 1, bởi 1, bởi 0 → vị trí của từ trong trường hợp đặc biệt.\" This translates to \"The meaning of the vector is: the position of the vector in the special case.\"\n\nBelow the diagram, there is a mathematical expression: \"Ví dụ: motel = [0 0 0 0 0 0 0 0 0 0 0] hotel = [0 0 0 0 0 0 0 0 0] 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 9 0 0 0 0 0 0 0 0 0 10 0 0 0 0 0 0 0 0 0 11 0 0 0 0 0 0 0 0 0 12 0 0 0 0 0 0 0 0 0 13 0 0 0 0 0 0 0 0 0 14 0 0 0 0 0 0 0 0 0 15 0 0 0 0 0 0 0 0 0 16 0 0 0 0 0 0 0 0 0 17 0 0 0 0 0 0 0 0 0 18 0 0 0 0 0 0 0 0 0 19 0 0 0 0 0 0 0 0 0 20 0 0 0 0 0 0 0 0 0 21 0 0 0 0 0 0 0 0 0 22 0 0 0 0 0 0 0 0 0 23 0 0 0 0 0 0 0 0 0 24 0 0 0 0 0 0 0 0 0 25 0 0 0 0 0 0 0 0 0 26 0 0 0 0 0 0 0 0 0 27 0 0 0 0 0 0 0 0 0 28 0 0 0 0 0 0 0 0 0 29 0 0 0 0 0 0 0 0 0 30 0 0 0 0 0 0 0 0 0 31 0 0 0 0 0 0 0 0 0 32 0 0 0 0 0 0 0 0 0 33 0 0 0 0 0 0 0 0 0 34 0 0 0 0 0 0 0 0 0 35 0 0 0 0 0 0 0 0 0 36 0 0 0 0 0 0 0 0 0 37 0 0 0 0 0 0 0 0 0 38 0 0 0 0 0 0 0 0 0 39 0 0 0 0 0 0 0 0 0 40 0 0 0 0 0 0 0 0 0 41 0 0 0 0 0 0 0 0 0 42 0 0 0 0 0 0 0 0 0 43 0 0 0 0 0 0 0 0 0 44 0 0 0 0 0 0 0 0 0 45 0 0 0 0 0 0 0 0 0 46 0 0 0 0 0 0 0 0 0 47 0 0 0 0 0 0 0 0 0 48 0 0 0 0 0 0 0 0 0 49 0 0 0 0 0 0 0 0 0 50 0 0 0 0 0 0 0 0 0 51 0 0 0 0 0 0 0 0 0 52 0 0 0 0 0 0 0 0 0 53 0 0 0 0 0 0 0 0 0 54 0 0 0 0 0 0 0 0 0 55 0 0 0 0 0 0 0 0 0 56 0 0 0 0 0 0 0 0 0 57 0 0 0 0 0 0 0 0 0 58 0 0 0 0 0 0 0 0 0 59 0 0 0 0 0 0 0 0 0 60 0 0 0 0 0 0 0 0 0 61 0 0 0 0 0 0 0 0 0 62 0 0 0 0 0 0 0 0 0 63 0 0 0 0 0 0 0 0 0 64 0 0 0 0 0 0 0 0 0 65 0 0 0 0 0 0 0 0 0 66 0 0 0 0 0 0 0 0 0 67 0 0 0 0 0 0 0 0 0 68 0 0 0 0 0 0 0 0 0 69 0 0 0 0 0 0 0 0 0 70 0 0 0 0 0 0 0 0 0 71 0 0 0 0 0 0 0 0 0 72 0 0 0 0 0 0 0 0 0 73 0 0 0 0 0 0 0 0 0 74 0 0 0 0 0 0 0 0 0 75 0 0 0 0 0 0 0 0 0 76 0 0 0 0 0 0 0 0 0 77 0 0 0 0 0 0 0 0 0 78 0 0 0 0 0 0 0 0 0 79 0 0 0 0 0 0 0 0 0 80 0 0 0 0 0 0 0 0 0 81 0 0 0 0 0 0 0 0 0 82 0 0 0 0 0 0 0 0 0 83 0 0 0 0 0 0 0 0 0 84 0 0 0 0 0 0 0 0 0 85 0 0 0 0 0 0 0 0 0 86 0 0 0 0 0 0 0 0 0 87 0 0 0 0 0 0 0 0 0 88 0 0 0 0 0 0 0 0 0 89 0 0 0 0 0 0 0 0 0 90 0 0 0 0 0 0 0 0 0 91 0 0 0 0 0 0 0 0 0 92 0 0 0 0 0 0 0 0 0 93 0 0 0 0 0 0 0 0 0 94 0 0 0 0 0 0 0 0 0 95 0 0 0 0 0 0 0 0 0 96 0 0 0 0 0 0 0 0 0 97 0 0 0 0 0 0 0 0 0 98 0 0 0 0 0 0 0 0 0 99 0 0 0 0 0 0 0 0 0 100 0 0 0 0 0 0 0 0 0 101 0 0 0 0 0 0 0 0 0 102 0 0 0 0 0 0 0 0 0 103 0 0 0 0 0 0 0 0 0 104 0 0 0 0 0 0 0 0 0 105 0 0 0 0 0 0 0 0 0 106 0 0 0 0 0 0 0 0 0 107 0 0 0 0 0 0 0 0 0 108 0 0 0 0 0 0 0 0 0 109 0 0 0 0 0 0 0 0 0 110 0 0 0 0 0 0 0 0 0 111 0 0 0 0 0 0 0 0 0 112 0 0 0 0 0 0 0 0 0 113 0 0 0 0 0 0 0 0 0 114 0 0 0 0 0 0 0 0 0 115 0 0 0 0 0 0 0 0 0 116 0 0 0 0 0 0 0 0 0 117 0 0 0 0 0 0 0 0 0 118 0 0 0 0 0 0 0 0 0 119 0 0 0 0 0 0 0 0 0 120 0 0 0 0 0 0 0 0 0 121 0 0 0 0 0 0 0 0 0 122 0 0 0 0 0 0 0 0 0 123 0 0 0 0 0 0 0 0 0 124 0 0 0 0 0 0 0 0 0 125 0 0 0 0 0 0 0 0 0 126 0 0 0 0 0 0 0 0 0 127 0 0 0 0 0 0 0 0 0 128 0 0 0 0 0 0 0 0 0 129 0 0 0 0 0 0 0 0 0 130 0 0 0 0 0 0 0 0 0 131 0 0 0 0 0 0 0 0 0 132 0 0 0 0 0 0 0 0 0 133 0 0 0 0 0 0 0 0 0 134 0 0 0 0 0 0 0 0 0 135 0 0 0 0 0 0 0 0 0 136 0 0 0 0 0 0 0 0 0 137 0 0 0 0 0 0 0 0 0 138 0 0 0 0 0 0 0 0 0 139 0 0 0 0 0 0 0 0 0 140 0 0 0 0 0 0 0 0 0 141 0 0 0 0 0 0 0 0 0 142 0 0 0 0 0 0 0 0 0 143 0 0 0 0 0 0 0 0 0 144 0 0 0 0 0 0 0 0 0 145 0 0 0 0 0 0 0 0 0 146 0 0 0 0 0 0 0 0 0 147 0 0 0 0 0 0 0 0 0 148 0 0 0 0 0 0 0 0 0 149 0 0 0 0 0 0 0 0 0 150 0 0 0 0 0 0 0 0 0 151 0 0 0 0 0 0 0 0 0 152 0 0 0 0 0 0 0 0 0 153 0 0 0 0 0 0 0 0 0 154 0 0 0 0 0 0 0 0 0 155 0 0 0 0 0 0 0 0 0 156 0 0 0 0 0 0 0 0 0 157 0 0 0 0 0 0 0 0 0 158 0 0 0 0 0 0 0 0 0 159 0 0 0 0 0 0 0 0 0 160 0 0 0 0 0 0 0 0 0 161 0 0 0 0 0 0 0 0 0 162 0 0 0 0 0 0 0 0 0 163 0 0 0 0 0 0 0 0 0 164 0 0 0 0 0 0 0 0 0 165 0 0 0 0 0 0 0 0 0 166 0 0 0 0 0 0 0 0 0 167 0 0 0 0 0 0 0 0 0 168 0 0 0 0 0 0 0 0 0 169 0 0 0 0 0 0 0 0 ","output_type":"stream"}],"execution_count":null}]}